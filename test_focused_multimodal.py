#!/usr/bin/env python3
"""
Focused test script for multimodal RAG with proper API key loading
"""
import asyncio
import os
import json
import logging
from dotenv import load_dotenv
from app.core.rag_pipeline import rag_pipeline
from app.core.config import settings

# Load environment variables
load_dotenv()

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

async def test_multimodal_rag():
    """Test multimodal RAG with image and table queries"""
    print("ğŸ” FOCUSED MULTIMODAL RAG TEST")
    print("=" * 50)
    
    # Print configuration
    print(f"ğŸ”§ LLM Provider: {settings.llm_provider}")
    print(f"ğŸ”§ OpenAI API Key: {settings.openai_api_key[:10]}..." if settings.openai_api_key else "âŒ No OpenAI key")
    print(f"ğŸ”§ DeepSeek API Key: {settings.deepseek_api_key[:10]}..." if settings.deepseek_api_key else "âŒ No DeepSeek key")
    print(f"ğŸ”§ Vision LLM: {settings.use_vision_llm}")
    print(f"ğŸ”§ Embedding Provider: {settings.embedding_provider}")
    
    # Test 1: Ingest document
    print("\nğŸ“„ STEP 1: INGESTING DOCUMENT...")
    result = await rag_pipeline.ingest_document(
        "attention_all_you_need.pdf",
        clear_existing=True
    )
    print(f"âœ… Ingestion: {result['message']}")
    print(f"ğŸ“Š Documents: {result['processing_stats']}")
    
    # Test 2: Image-based query
    print("\nğŸ–¼ï¸ STEP 2: TESTING IMAGE QUERY...")
    image_query = "Show me the transformer architecture diagram and explain what you see"
    
    result = await rag_pipeline.query(
        query=image_query,
        search_type="hybrid",
        k=8,
        include_images=True
    )
    
    sources = result.get('sources', [])
    image_sources = [s for s in sources if s.get('content_type') == 'image']
    
    print(f"ğŸ“Š Total sources: {len(sources)}")
    print(f"ğŸ–¼ï¸ Image sources: {len(image_sources)}")
    print(f"ğŸ‘ï¸ Vision LLM used: {result.get('used_vision_llm', False)}")
    print(f"ğŸ–¼ï¸ Images in context: {result.get('images_in_context', 0)}")
    print(f"âœ… Success: {result.get('success', False)}")
    
    if image_sources:
        print("\nğŸ” IMAGE SOURCES FOUND:")
        for i, img in enumerate(image_sources[:3]):
            print(f"  {i+1}. Page {img.get('page')}: {img.get('content', '')[:80]}...")
            print(f"     Path: {img.get('image_path', 'N/A')}")
            print(f"     URL: {img.get('image_url', 'N/A')}")
    
    print(f"\nğŸ¤– ANSWER ({len(result.get('answer', ''))} chars):")
    print(f"{result.get('answer', 'No answer')[:300]}...")
    
    # Test 3: Table-based query
    print("\nğŸ“Š STEP 3: TESTING TABLE QUERY...")
    table_query = "What are the performance metrics and BLEU scores in the results table?"
    
    result = await rag_pipeline.query(
        query=table_query,
        search_type="hybrid",
        k=5,
        include_images=True
    )
    
    print(f"ğŸ“Š Sources found: {len(result.get('sources', []))}")
    print(f"ğŸ¤– Answer: {result.get('answer', 'No answer')[:200]}...")
    
    # Test 4: Architecture query with vision
    print("\nğŸ—ï¸ STEP 4: TESTING ARCHITECTURE QUERY...")
    arch_query = "Describe the multi-head attention mechanism using the diagram"
    
    result = await rag_pipeline.query(
        query=arch_query,
        search_type="multimodal",
        k=6,
        include_images=True
    )
    
    print(f"ğŸ“Š Sources: {len(result.get('sources', []))}")
    print(f"ğŸ‘ï¸ Vision LLM: {result.get('used_vision_llm', False)}")
    print(f"ğŸ–¼ï¸ Images in context: {result.get('images_in_context', 0)}")
    print(f"ğŸ¤– Answer: {result.get('answer', 'No answer')[:200]}...")
    
    # Save results
    test_results = {
        'configuration': {
            'llm_provider': settings.llm_provider,
            'has_openai_key': bool(settings.openai_api_key and not settings.openai_api_key.startswith('your_')),
            'has_deepseek_key': bool(settings.deepseek_api_key and not settings.deepseek_api_key.startswith('your_')),
            'vision_enabled': settings.use_vision_llm,
            'embedding_provider': settings.embedding_provider
        },
        'tests': {
            'image_query': {
                'query': image_query,
                'total_sources': len(sources),
                'image_sources': len(image_sources),
                'vision_llm_used': result.get('used_vision_llm', False),
                'images_in_context': result.get('images_in_context', 0),
                'success': result.get('success', False),
                'answer_length': len(result.get('answer', ''))
            }
        }
    }
    
    with open('focused_test_results.json', 'w') as f:
        json.dump(test_results, f, indent=2)
    
    print(f"\nğŸ’¾ Results saved to: focused_test_results.json")
    print("âœ… Test complete!")

if __name__ == "__main__":
    asyncio.run(test_multimodal_rag())
